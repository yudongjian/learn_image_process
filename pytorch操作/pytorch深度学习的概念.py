import torch

'''
权重参数:
    autograd 机制
    求导的作用：  是对神经网络的权重参数进行调整
    导数  即为变化率，通过 结果 和 输入，计算出变化率--->权重参数


损失函数:
    用于计算 标签值 和 预测值 之间差异的函数
    eg 距离向量，绝对向量
    
    torch.nn as nn
    nn.L1Loss: 绝对误差的平均值
    ......


优化器:
    优化器用通俗的话来说就是一种算法，是一种计算导数的算法
    最好的优化器就是每一轮样本数据的优化都让权重参数匀速的接近目标值，而不是忽上忽下跳跃的变化。
    pytorch 的优化器都放在 torch.optim 包中。
    常见的优化器有：
        SGD，Adam，Adadelta，Adagrad，Adamax 等。===  SGD即随机梯度下降
        这几种优化器足够现实世界中使用了，如果需要定制特殊的优化器，pytorch 也提供了定制化的手段。

学习率: 
    step，即沿着梯度方向的步长，，它决定了模型在每次迭代中更新权重的速度
    学习率越大，权重更新的幅度就越大，模型训练的速度也会变快。
    然而，如果学习率过大，就有可能错过最优解，甚至会导致模型无法收敛。
    相反，如果学习率过小，模型训练的速度会很慢，可能需要更多的迭代次数来达到最优解。
    因此，选择一个合适的学习率是非常重要的，通常需要进行实验来找到最佳值
    做法：开始的时候设置的大一点，最后的时候设置的小一点。
    
线性回归

梯度清零：
    是指在神经网络的反向传播过程中，将之前计算得到的梯度归零，以避免之前的梯度对当前的梯度更新产生干扰。
    梯度清零通常在每次更新参数前进行，可以提高梯度下降算法的收敛速度和精度。


过程：
    前向传播 ----> 权重参数 ----> 损失函数 -->对比标签
    反向传播 ---> 更新权重参数

'''


'''
tensor

    scalar  数值  0维度
    
    vector  通常指 特征。 向量 1维度
    
    matrix   矩阵 2维度
    
    n维度     3维度以上


'''

# 设置可以求导的两种方式
a = torch.randn(3,4)
a.requires_grad = True
print(a)

b = torch.ones(3,4,requires_grad=True)
print(b)

s = b + b


z = s*s*3

print(z)
z.backward(torch.ones_like(z)) # 反向传播

print(b.grad)

#
#
# # 是否是传播链路上面的叶子
# print(b.is_leaf)
# print(s.is_leaf)
